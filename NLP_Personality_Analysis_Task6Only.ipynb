{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9247e3",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "## Create and split training/testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b819709",
   "metadata": {},
   "source": [
    "### Import frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56141cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a31c1663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ee644",
   "metadata": {},
   "source": [
    "### Load Dataset and split labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d446266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dyadic_PELD.tsv', sep='\\t', header=0)\n",
    "\n",
    "\n",
    "labels = df['Personality'].to_numpy()\n",
    "labels = [eval(x) for x in labels]\n",
    "df_labels = pd.DataFrame(labels, columns=['Openness', 'Conscientiousness', 'Extroversion', 'Agreeableness', 'Neuroticism'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd18fb3",
   "metadata": {},
   "source": [
    "### Function for creating training datasets with different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c8ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_labels(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True):\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(n_gram,n_gram))\n",
    "\n",
    "\n",
    "    # Utterance 1 is always True\n",
    "    utterance_tfidf = tfidf.fit_transform(df['Utterance_1'])\n",
    "    df_features = pd.DataFrame(utterance_tfidf.toarray(), columns=[f\"Utterance1_{word}\" for word in tfidf.get_feature_names_out()])\n",
    "    if emotion:\n",
    "        df_emotions = pd.get_dummies(df[['Emotion_1']])\n",
    "        df_features = pd.concat([df_features, df_emotions], axis=1)\n",
    "    if sentiment:\n",
    "        df_sentiments = pd.get_dummies(df[['Sentiment_1']])\n",
    "        df_features = pd.concat([df_features, df_sentiments], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    if utterance_2:\n",
    "        utterance_tfidf = tfidf.fit_transform(df['Utterance_2'])\n",
    "        df_utterance_tfidf = pd.DataFrame(utterance_tfidf.toarray(), columns=[f\"Utterance2_{word}\" for word in tfidf.get_feature_names_out()])\n",
    "        df_features = pd.concat([df_features, df_utterance_tfidf], axis=1)\n",
    "        if emotion:\n",
    "            df_emotions = pd.get_dummies(df[['Emotion_2']])\n",
    "            df_features = pd.concat([df_features, df_emotions], axis=1)\n",
    "        if sentiment:\n",
    "            df_sentiments = pd.get_dummies(df[['Sentiment_2']])\n",
    "            df_features = pd.concat([df_features, df_sentiments], axis=1)\n",
    "\n",
    "\n",
    "    if utterance_3:\n",
    "        utterance_tfidf = tfidf.fit_transform(df['Utterance_3'])\n",
    "        df_utterance_tfidf = pd.DataFrame(utterance_tfidf.toarray(), columns=[f\"Utterance3_{word}\" for word in tfidf.get_feature_names_out()])\n",
    "        df_features = pd.concat([df_features, df_utterance_tfidf], axis=1)\n",
    "        if emotion:\n",
    "            df_emotions = pd.get_dummies(df[['Emotion_3']])\n",
    "            df_features = pd.concat([df_features, df_emotions], axis=1)\n",
    "        if sentiment:\n",
    "            df_sentiments = pd.get_dummies(df[['Sentiment_3']])\n",
    "            df_features = pd.concat([df_features, df_sentiments], axis=1)\n",
    "\n",
    "    # Train-test split\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(df_features, df_labels, test_size=0.2, random_state=42, stratify=df['Speaker_1'])\n",
    "\n",
    "    return features_train, features_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc183f8",
   "metadata": {},
   "source": [
    "### Setup 6 different ai models to do parameter hypertuning with gridsearch on\n",
    "This is so we can check which models performs best on our dataset, with several different parameters.\n",
    "\n",
    "Note, for now we only use deep learning as this takes a long time to search, it is also unnecesary to hyper optimize for this assignment as the course is about Processing Natural Language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a3eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deep learning model function\n",
    "def create_deep_learning_model(input_dim, dense_units=512, dropout_rate=0.5):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),  # Define the input layer with the shape\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units // 2, activation='relu'),\n",
    "        Dense(5, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "def grid_search_deep_learning(features_train, labels_train):\n",
    "    model = KerasRegressor(\n",
    "        model=create_deep_learning_model,\n",
    "        input_dim=features_train.shape[1],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Define the param_grid with parameter names directly available in KerasRegressor\n",
    "    param_grid = {\n",
    "        'model__dense_units': [512, 256],\n",
    "        'model__dropout_rate': [0.3, 0.5],\n",
    "        'epochs': [10],\n",
    "        'batch_size': [16, 32]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    print(\"Best Deep Learning Params:\", grid_search.best_params_)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# Linear regression model function\n",
    "def grid_search_linear_regression(features_train, labels_train):\n",
    "    model = LinearRegression()\n",
    "    multi_target_lr = MultiOutputRegressor(model)  # Wrap in MultiOutputRegressor\n",
    "    param_grid = {}\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=multi_target_lr, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    print(\"Best Linear Regression Params:\", grid_search.best_params_)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# Polynomial regression model function\n",
    "def grid_search_polynomial_regression(features_train, labels_train):\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    multi_target_poly = MultiOutputRegressor(model)  # Wrap in MultiOutputRegressor\n",
    "    param_grid = {\n",
    "        'estimator__poly__degree': [2, 3]  # Adjust the parameter for the pipeline\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=multi_target_poly, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    print(\"Best Polynomial Regression Params:\", grid_search.best_params_)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# SVR model function\n",
    "def grid_search_svr(features_train, labels_train):\n",
    "    # Initialize SVR model\n",
    "    model = SVR()\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'estimator__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'estimator__C': [0.1, 1, 10],\n",
    "        'estimator__epsilon': [0.01, 0.1, 1]\n",
    "    }\n",
    "    \n",
    "    # Use MultiOutputRegressor with GridSearchCV\n",
    "    multi_target_svr = MultiOutputRegressor(model)\n",
    "    grid_search = GridSearchCV(estimator=multi_target_svr, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    \n",
    "    print(\"Best SVR Params:\", grid_search.best_params_)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# Decision tree model function\n",
    "def grid_search_decision_tree(features_train, labels_train):\n",
    "    model = DecisionTreeRegressor()\n",
    "    param_grid = {\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10, 16]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    print(\"Best Decision Tree Params:\", grid_search.best_params_)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# Random forest model function\n",
    "def grid_search_random_forest(features_train, labels_train):\n",
    "    model = RandomForestRegressor()\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    print(\"Best Random Forest Params:\", grid_search.best_params_)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6c8e7",
   "metadata": {},
   "source": [
    "### Run grid search on all models\n",
    "Note, as specified earlier, there is not enough time for this task that doesn't contribute to the field of NLP.\n",
    "\n",
    "Therefore, we comment it out for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9633dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(features_train.shape)\n",
    "\n",
    "# # Linear regression model\n",
    "# best_linear_model, best_params_lr, best_params_lr_repeat, best_score_lr = grid_search_linear_regression(features_train, labels_train)\n",
    "# print(f\"Best Linear Model: {best_linear_model}\")\n",
    "# print(f\"Best Parameters (Linear Regression): {best_params_lr}\")\n",
    "# print(f\"Repeated Best Parameters (Linear Regression): {best_params_lr_repeat}\")\n",
    "# print(f\"Best Score (Linear Regression): {best_score_lr}\")\n",
    "\n",
    "# # Polynomial regression model\n",
    "# best_polynomial_model, best_params_poly, best_params_poly_repeat, best_score_poly = grid_search_polynomial_regression(features_train, labels_train)\n",
    "# print(f\"Best Polynomial Model: {best_polynomial_model}\")\n",
    "# print(f\"Best Parameters (Polynomial Regression): {best_params_poly}\")\n",
    "# print(f\"Repeated Best Parameters (Polynomial Regression): {best_params_poly_repeat}\")\n",
    "# print(f\"Best Score (Polynomial Regression): {best_score_poly}\")\n",
    "\n",
    "# # for dataframe in df_features_array:\n",
    "\n",
    "# # SVR model\n",
    "# # Train-test split\n",
    "# features_train, features_test, labels_train, labels_test = train_test_split(dataframe, df_labels, test_size=0.2, random_state=42, stratify=df['Speaker_1'])\n",
    "# best_svr_model, best_params_svr, best_params_svr_repeat, best_score_svr = grid_search_svr(features_train, labels_train)\n",
    "# print(f\"Best SVR Model: {best_svr_model}\")\n",
    "# print(f\"Best Parameters (SVR): {best_params_svr}\")\n",
    "# print(f\"Repeated Best Parameters (SVR): {best_params_svr_repeat}\")\n",
    "# print(f\"Best Score (SVR): {best_score_svr}\")\n",
    "\n",
    "# # Decision tree model\n",
    "# best_decision_tree_model, best_params_dt, best_params_dt_repeat, best_score_dt = grid_search_decision_tree(features_train, labels_train)\n",
    "# print(f\"Best Decision Tree Model: {best_decision_tree_model}\")\n",
    "# print(f\"Best Parameters (Decision Tree): {best_params_dt}\")\n",
    "# print(f\"Repeated Best Parameters (Decision Tree): {best_params_dt_repeat}\")\n",
    "# print(f\"Best Score (Decision Tree): {best_score_dt}\")\n",
    "\n",
    "# # Random forest model\n",
    "# best_random_forest_model, best_params_rf, best_params_rf_repeat, best_score_rf = grid_search_random_forest(features_train, labels_train)\n",
    "# print(f\"Best Random Forest Model: {best_random_forest_model}\")\n",
    "# print(f\"Best Parameters (Random Forest): {best_params_rf}\")\n",
    "# print(f\"Repeated Best Parameters (Random Forest): {best_params_rf_repeat}\")\n",
    "# print(f\"Best Score (Random Forest): {best_score_rf}\")\n",
    "\n",
    "# # Deep learning model\n",
    "# best_deep_learning_model, best_params_dl, best_params_dl_repeat, best_score_dl = grid_search_deep_learning(features_train, labels_train)\n",
    "# print(f\"Best Deep Learning Model: {best_deep_learning_model}\")\n",
    "# print(f\"Best Parameters (Deep Learning): {best_params_dl}\")\n",
    "# print(f\"Repeated Best Parameters (Deep Learning): {best_params_dl_repeat}\")\n",
    "# print(f\"Best Score (Deep Learning): {best_score_dl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb9e97",
   "metadata": {},
   "source": [
    "### Run grid search only one a deep learning model\n",
    "Therefore we only run grid search on deep learning, and not all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f597fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Deep Learning Params: {'batch_size': 32, 'epochs': 10, 'model__dense_units': 256, 'model__dropout_rate': 0.5}\n",
      "Best Deep Learning Model: KerasRegressor(\n",
      "\tmodel=<function create_deep_learning_model at 0x000001DB6D5D9620>\n",
      "\tbuild_fn=None\n",
      "\twarm_start=False\n",
      "\trandom_state=None\n",
      "\toptimizer=rmsprop\n",
      "\tloss=None\n",
      "\tmetrics=None\n",
      "\tbatch_size=32\n",
      "\tvalidation_batch_size=None\n",
      "\tverbose=0\n",
      "\tcallbacks=None\n",
      "\tvalidation_split=0.0\n",
      "\tshuffle=True\n",
      "\trun_eagerly=False\n",
      "\tepochs=10\n",
      "\tinput_dim=12262\n",
      "\tmodel__dense_units=256\n",
      "\tmodel__dropout_rate=0.5\n",
      ")\n",
      "Best Parameters (Deep Learning): {'batch_size': 32, 'epochs': 10, 'model__dense_units': 256, 'model__dropout_rate': 0.5}\n",
      "Repeated Best Parameters (Deep Learning): {'batch_size': 32, 'epochs': 10, 'model__dense_units': 256, 'model__dropout_rate': 0.5}\n",
      "Best Score (Deep Learning): -0.005872741745173176\n"
     ]
    }
   ],
   "source": [
    "features_train, features_test, labels_train, labels_test = get_training_labels()\n",
    "\n",
    "\n",
    "# Deep learning model\n",
    "best_deep_learning_model, best_params_dl, best_params_dl_repeat, best_score_dl = grid_search_deep_learning(features_train, labels_train)\n",
    "print(f\"Best Deep Learning Model: {best_deep_learning_model}\")\n",
    "print(f\"Best Parameters (Deep Learning): {best_params_dl}\")\n",
    "print(f\"Repeated Best Parameters (Deep Learning): {best_params_dl_repeat}\")\n",
    "print(f\"Best Score (Deep Learning): {best_score_dl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a9f81f",
   "metadata": {},
   "source": [
    "The best deep learning parameters for this problem are these:\n",
    "\n",
    "batch_size: 32\n",
    "\n",
    "epochs: 10\n",
    "\n",
    "model__dense_units: 256\n",
    "\n",
    "model__dropout_rate: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6f6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'batch_size': 32, 'epochs': 10, 'model__dense_units': 256, 'model__dropout_rate': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc17a93",
   "metadata": {},
   "source": [
    "### Create a list of datasets that differs in simple terms\n",
    "This is the meat of this task. We want to determine how different features affect the model.\n",
    "\n",
    "Therefore we start by making only one or two changes in each dataset in order to be able to measure the impact of every single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a6ebbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_datasets = {\n",
    "    \"all_data\": list(get_training_labels(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"5000features\": list(get_training_labels(max_features=5000, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"1000features\": list(get_training_labels(max_features=1000, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"bigram\": list(get_training_labels(max_features=None, n_gram=2, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"trigram\": list(get_training_labels(max_features=None, n_gram=3, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"quadgram\": list(get_training_labels(max_features=None, n_gram=4, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"no_sentiment\": list(get_training_labels(max_features=None, n_gram=1, emotion=True, sentiment=False, utterance_2=True, utterance_3=True)),\n",
    "    \"no_emotion\": list(get_training_labels(max_features=None, n_gram=1, emotion=False, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"no_emotion_or_sentiment\": list(get_training_labels(max_features=None, n_gram=1, emotion=False, sentiment=False, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"no_utterance2\": list(get_training_labels(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=False, utterance_3=True)),\n",
    "    \"only_utterance1\": list(get_training_labels(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=False, utterance_3=False))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6931b6",
   "metadata": {},
   "source": [
    "### Create a function for running the model on all datasets\n",
    "We split the 'train' into 'train/val', so that we have training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "386103ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_learning_model_with_params(features_train, features_test, labels_train, labels_test, params):\n",
    "    # Extract parameters from the dictionary\n",
    "    dense_units = params.get('model__dense_units', 512)\n",
    "    dropout_rate = params.get('model__dropout_rate', 0.5)\n",
    "    epochs = params.get('epochs', 10)\n",
    "    batch_size = params.get('batch_size', 32)\n",
    "    \n",
    "    # Train-val split\n",
    "    df_full_train, df_full_test, labels_train, labels_test = train_test_split(df, df_labels, test_size=0.2, random_state=42, stratify=df['Speaker_1'])\n",
    "    features_inner_train, features_val, labels_inner_train, labels_val = train_test_split(features_train, labels_train, test_size=0.2, random_state=42, stratify=df_full_train['Speaker_1'])\n",
    "\n",
    "    # Define the model architecture based on chosen parameters\n",
    "    model = Sequential([\n",
    "        Input(shape=(features_train.shape[1],)),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units // 2, activation='relu'),\n",
    "        Dense(5, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss='mse', metrics=['mae', 'mape'])\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(features_inner_train, labels_inner_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(features_val, labels_val),\n",
    "                        verbose=1)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_mse, test_mae, test_mape = model.evaluate(features_test, labels_test)\n",
    "    \n",
    "    return test_mse, test_mae, test_mape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f761ce",
   "metadata": {},
   "source": [
    "### Run the model on every dataset to compare test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a7648de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: all_data\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - loss: 0.0554 - mae: 0.1728 - mape: 34.4814 - val_loss: 0.0071 - val_mae: 0.0679 - val_mape: 14.1835\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0088 - mae: 0.0752 - mape: 15.5987 - val_loss: 0.0076 - val_mae: 0.0714 - val_mape: 14.4054\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0069 - mae: 0.0661 - mape: 13.6445 - val_loss: 0.0064 - val_mae: 0.0634 - val_mape: 13.2364\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0049 - mae: 0.0558 - mape: 11.5159 - val_loss: 0.0066 - val_mae: 0.0634 - val_mape: 12.8835\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0033 - mae: 0.0458 - mape: 9.4348 - val_loss: 0.0063 - val_mae: 0.0622 - val_mape: 12.8447\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0025 - mae: 0.0390 - mape: 8.0411 - val_loss: 0.0060 - val_mae: 0.0597 - val_mape: 12.4248\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0019 - mae: 0.0344 - mape: 7.1167 - val_loss: 0.0057 - val_mae: 0.0580 - val_mape: 12.2464\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0016 - mae: 0.0315 - mape: 6.5386 - val_loss: 0.0058 - val_mae: 0.0584 - val_mape: 12.1737\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0015 - mae: 0.0297 - mape: 6.1284 - val_loss: 0.0058 - val_mae: 0.0582 - val_mape: 12.1949\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0013 - mae: 0.0277 - mape: 5.7105 - val_loss: 0.0059 - val_mae: 0.0583 - val_mape: 12.1642\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0060 - mae: 0.0593 - mape: 12.4821\n",
      "all_data:\n",
      "Mean Squared Error: 0.00585175072774291\n",
      "Mean Absolute Error: 0.05871236324310303\n",
      "Mean Absolute Percentage Error: 12.269471168518066\n",
      "Model: 5000features\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - loss: 0.0529 - mae: 0.1679 - mape: 33.2510 - val_loss: 0.0072 - val_mae: 0.0672 - val_mape: 13.6484\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0090 - mae: 0.0762 - mape: 15.7357 - val_loss: 0.0074 - val_mae: 0.0694 - val_mape: 13.9423\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0066 - mae: 0.0648 - mape: 13.3789 - val_loss: 0.0079 - val_mae: 0.0722 - val_mape: 14.4001\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0050 - mae: 0.0557 - mape: 11.4803 - val_loss: 0.0073 - val_mae: 0.0674 - val_mape: 13.5921\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0034 - mae: 0.0458 - mape: 9.4387 - val_loss: 0.0061 - val_mae: 0.0605 - val_mape: 12.4886\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0025 - mae: 0.0392 - mape: 8.1343 - val_loss: 0.0061 - val_mae: 0.0607 - val_mape: 12.4958\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0020 - mae: 0.0347 - mape: 7.1863 - val_loss: 0.0060 - val_mae: 0.0593 - val_mape: 12.4250\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0017 - mae: 0.0320 - mape: 6.6294 - val_loss: 0.0059 - val_mae: 0.0587 - val_mape: 12.2648\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0015 - mae: 0.0303 - mape: 6.2734 - val_loss: 0.0060 - val_mae: 0.0588 - val_mape: 12.2535\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0013 - mae: 0.0279 - mape: 5.7662 - val_loss: 0.0058 - val_mae: 0.0581 - val_mape: 12.2099\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0060 - mae: 0.0590 - mape: 12.4791\n",
      "5000features:\n",
      "Mean Squared Error: 0.00571276992559433\n",
      "Mean Absolute Error: 0.05788521096110344\n",
      "Mean Absolute Percentage Error: 12.178739547729492\n",
      "Model: 1000features\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 0.0358 - mae: 0.1402 - mape: 28.1031 - val_loss: 0.0082 - val_mae: 0.0720 - val_mape: 14.3430\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0090 - mae: 0.0760 - mape: 15.7540 - val_loss: 0.0085 - val_mae: 0.0732 - val_mape: 14.2698\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0068 - mae: 0.0658 - mape: 13.6537 - val_loss: 0.0073 - val_mae: 0.0684 - val_mape: 13.7368\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0051 - mae: 0.0570 - mape: 11.8722 - val_loss: 0.0065 - val_mae: 0.0642 - val_mape: 13.3500\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0037 - mae: 0.0483 - mape: 10.0957 - val_loss: 0.0064 - val_mae: 0.0621 - val_mape: 12.8167\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0029 - mae: 0.0424 - mape: 8.9067 - val_loss: 0.0061 - val_mae: 0.0604 - val_mape: 12.6784\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0025 - mae: 0.0388 - mape: 8.1524 - val_loss: 0.0061 - val_mae: 0.0602 - val_mape: 12.6590\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.0021 - mae: 0.0358 - mape: 7.4755 - val_loss: 0.0060 - val_mae: 0.0596 - val_mape: 12.5736\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0019 - mae: 0.0343 - mape: 7.1749 - val_loss: 0.0059 - val_mae: 0.0596 - val_mape: 12.5511\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.0017 - mae: 0.0317 - mape: 6.6145 - val_loss: 0.0060 - val_mae: 0.0598 - val_mape: 12.4939\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0061 - mae: 0.0601 - mape: 12.6856\n",
      "1000features:\n",
      "Mean Squared Error: 0.0059440056793391705\n",
      "Mean Absolute Error: 0.059513580054044724\n",
      "Mean Absolute Percentage Error: 12.483930587768555\n",
      "Model: bigram\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 128ms/step - loss: 0.0651 - mae: 0.1878 - mape: 37.1938 - val_loss: 0.0068 - val_mae: 0.0657 - val_mape: 13.9552\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 121ms/step - loss: 0.0118 - mae: 0.0874 - mape: 17.6953 - val_loss: 0.0073 - val_mae: 0.0693 - val_mape: 13.8728\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 121ms/step - loss: 0.0073 - mae: 0.0681 - mape: 13.8064 - val_loss: 0.0067 - val_mae: 0.0629 - val_mape: 12.7293\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 119ms/step - loss: 0.0046 - mae: 0.0541 - mape: 10.9744 - val_loss: 0.0072 - val_mae: 0.0669 - val_mape: 13.2044\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 120ms/step - loss: 0.0032 - mae: 0.0445 - mape: 9.0681 - val_loss: 0.0061 - val_mae: 0.0596 - val_mape: 12.2276\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 120ms/step - loss: 0.0024 - mae: 0.0385 - mape: 7.8645 - val_loss: 0.0058 - val_mae: 0.0589 - val_mape: 12.2729\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 120ms/step - loss: 0.0019 - mae: 0.0339 - mape: 6.9585 - val_loss: 0.0056 - val_mae: 0.0569 - val_mape: 11.9930\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 118ms/step - loss: 0.0016 - mae: 0.0309 - mape: 6.3456 - val_loss: 0.0056 - val_mae: 0.0573 - val_mape: 12.0655\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 119ms/step - loss: 0.0014 - mae: 0.0286 - mape: 5.8511 - val_loss: 0.0057 - val_mae: 0.0579 - val_mape: 12.0577\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 120ms/step - loss: 0.0012 - mae: 0.0269 - mape: 5.5214 - val_loss: 0.0056 - val_mae: 0.0571 - val_mape: 11.9658\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0059 - mae: 0.0589 - mape: 12.4282\n",
      "bigram:\n",
      "Mean Squared Error: 0.005821918603032827\n",
      "Mean Absolute Error: 0.05843406170606613\n",
      "Mean Absolute Percentage Error: 12.250519752502441\n",
      "Model: trigram\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 179ms/step - loss: 0.0612 - mae: 0.1798 - mape: 35.9736 - val_loss: 0.0072 - val_mae: 0.0669 - val_mape: 14.4049\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - loss: 0.0145 - mae: 0.0960 - mape: 19.3849 - val_loss: 0.0066 - val_mae: 0.0645 - val_mape: 13.1956\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - loss: 0.0076 - mae: 0.0691 - mape: 14.0632 - val_loss: 0.0080 - val_mae: 0.0737 - val_mape: 14.4632\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - loss: 0.0046 - mae: 0.0537 - mape: 10.9125 - val_loss: 0.0068 - val_mae: 0.0663 - val_mape: 13.2350\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - loss: 0.0033 - mae: 0.0450 - mape: 9.2264 - val_loss: 0.0063 - val_mae: 0.0641 - val_mape: 13.1221\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - loss: 0.0025 - mae: 0.0391 - mape: 8.0507 - val_loss: 0.0057 - val_mae: 0.0584 - val_mape: 12.1932\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - loss: 0.0021 - mae: 0.0354 - mape: 7.3012 - val_loss: 0.0057 - val_mae: 0.0584 - val_mape: 12.2179\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - loss: 0.0017 - mae: 0.0320 - mape: 6.5957 - val_loss: 0.0056 - val_mae: 0.0572 - val_mape: 12.0122\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 171ms/step - loss: 0.0015 - mae: 0.0295 - mape: 6.0390 - val_loss: 0.0055 - val_mae: 0.0567 - val_mape: 12.0285\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - loss: 0.0013 - mae: 0.0276 - mape: 5.6445 - val_loss: 0.0056 - val_mae: 0.0566 - val_mape: 11.8815\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0058 - mae: 0.0582 - mape: 12.3555\n",
      "trigram:\n",
      "Mean Squared Error: 0.005729729309678078\n",
      "Mean Absolute Error: 0.05766170471906662\n",
      "Mean Absolute Percentage Error: 12.133415222167969\n",
      "Model: quadgram\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 166ms/step - loss: 0.0516 - mae: 0.1637 - mape: 32.6612 - val_loss: 0.0070 - val_mae: 0.0641 - val_mape: 13.1296\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 156ms/step - loss: 0.0133 - mae: 0.0916 - mape: 18.4152 - val_loss: 0.0083 - val_mae: 0.0756 - val_mape: 14.8463\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 157ms/step - loss: 0.0069 - mae: 0.0657 - mape: 13.3592 - val_loss: 0.0083 - val_mae: 0.0745 - val_mape: 14.4263\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 156ms/step - loss: 0.0042 - mae: 0.0514 - mape: 10.4501 - val_loss: 0.0073 - val_mae: 0.0684 - val_mape: 13.5329\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 156ms/step - loss: 0.0031 - mae: 0.0439 - mape: 8.9715 - val_loss: 0.0060 - val_mae: 0.0603 - val_mape: 12.4124\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 156ms/step - loss: 0.0025 - mae: 0.0382 - mape: 7.8733 - val_loss: 0.0056 - val_mae: 0.0573 - val_mape: 12.1075\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 157ms/step - loss: 0.0020 - mae: 0.0342 - mape: 7.0479 - val_loss: 0.0058 - val_mae: 0.0590 - val_mape: 12.2386\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 157ms/step - loss: 0.0016 - mae: 0.0312 - mape: 6.4417 - val_loss: 0.0055 - val_mae: 0.0560 - val_mape: 11.9430\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 155ms/step - loss: 0.0015 - mae: 0.0298 - mape: 6.1640 - val_loss: 0.0057 - val_mae: 0.0575 - val_mape: 12.0853\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 155ms/step - loss: 0.0013 - mae: 0.0272 - mape: 5.5979 - val_loss: 0.0056 - val_mae: 0.0567 - val_mape: 12.0783\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0058 - mae: 0.0586 - mape: 12.6247\n",
      "quadgram:\n",
      "Mean Squared Error: 0.005730041302740574\n",
      "Mean Absolute Error: 0.05791456997394562\n",
      "Mean Absolute Percentage Error: 12.371557235717773\n",
      "Model: no_sentiment\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0644 - mae: 0.1880 - mape: 37.3393 - val_loss: 0.0070 - val_mae: 0.0669 - val_mape: 14.1039\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0090 - mae: 0.0756 - mape: 15.5202 - val_loss: 0.0073 - val_mae: 0.0682 - val_mape: 13.9143\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0067 - mae: 0.0651 - mape: 13.3018 - val_loss: 0.0073 - val_mae: 0.0677 - val_mape: 13.4941\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0046 - mae: 0.0540 - mape: 11.0581 - val_loss: 0.0070 - val_mae: 0.0668 - val_mape: 13.5305\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0035 - mae: 0.0468 - mape: 9.5907 - val_loss: 0.0066 - val_mae: 0.0626 - val_mape: 12.6581\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0025 - mae: 0.0396 - mape: 8.1132 - val_loss: 0.0065 - val_mae: 0.0614 - val_mape: 12.4406\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0021 - mae: 0.0358 - mape: 7.3767 - val_loss: 0.0059 - val_mae: 0.0590 - val_mape: 12.4285\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0018 - mae: 0.0327 - mape: 6.7720 - val_loss: 0.0057 - val_mae: 0.0576 - val_mape: 12.1687\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0016 - mae: 0.0308 - mape: 6.3703 - val_loss: 0.0058 - val_mae: 0.0581 - val_mape: 12.1813\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0014 - mae: 0.0291 - mape: 6.0077 - val_loss: 0.0058 - val_mae: 0.0577 - val_mape: 12.0471\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0059 - mae: 0.0589 - mape: 12.4229\n",
      "no_sentiment:\n",
      "Mean Squared Error: 0.005785708781331778\n",
      "Mean Absolute Error: 0.05831199511885643\n",
      "Mean Absolute Percentage Error: 12.225582122802734\n",
      "Model: no_emotion\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0576 - mae: 0.1758 - mape: 34.7767 - val_loss: 0.0073 - val_mae: 0.0684 - val_mape: 14.0788\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0088 - mae: 0.0756 - mape: 15.5780 - val_loss: 0.0073 - val_mae: 0.0684 - val_mape: 13.7385\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0067 - mae: 0.0657 - mape: 13.5592 - val_loss: 0.0079 - val_mae: 0.0701 - val_mape: 13.8125\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0048 - mae: 0.0549 - mape: 11.2830 - val_loss: 0.0070 - val_mae: 0.0653 - val_mape: 13.1358\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0035 - mae: 0.0466 - mape: 9.5822 - val_loss: 0.0061 - val_mae: 0.0606 - val_mape: 12.6186\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0025 - mae: 0.0392 - mape: 8.0730 - val_loss: 0.0061 - val_mae: 0.0600 - val_mape: 12.5951\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0020 - mae: 0.0349 - mape: 7.2360 - val_loss: 0.0060 - val_mae: 0.0594 - val_mape: 12.4016\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0017 - mae: 0.0315 - mape: 6.5310 - val_loss: 0.0059 - val_mae: 0.0585 - val_mape: 12.3028\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0014 - mae: 0.0291 - mape: 6.0297 - val_loss: 0.0060 - val_mae: 0.0588 - val_mape: 12.2876\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0013 - mae: 0.0281 - mape: 5.8102 - val_loss: 0.0059 - val_mae: 0.0582 - val_mape: 12.2915\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0059 - mae: 0.0588 - mape: 12.5261\n",
      "no_emotion:\n",
      "Mean Squared Error: 0.005732996389269829\n",
      "Mean Absolute Error: 0.05790345370769501\n",
      "Mean Absolute Percentage Error: 12.254773139953613\n",
      "Model: no_emotion_or_sentiment\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - loss: 0.0737 - mae: 0.2062 - mape: 40.6543 - val_loss: 0.0081 - val_mae: 0.0723 - val_mape: 15.0718\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0091 - mae: 0.0761 - mape: 15.6081 - val_loss: 0.0074 - val_mae: 0.0691 - val_mape: 14.1920\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0057 - mae: 0.0599 - mape: 12.2950 - val_loss: 0.0069 - val_mae: 0.0650 - val_mape: 13.2360\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0036 - mae: 0.0477 - mape: 9.7953 - val_loss: 0.0064 - val_mae: 0.0616 - val_mape: 12.6360\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - loss: 0.0026 - mae: 0.0396 - mape: 8.1981 - val_loss: 0.0061 - val_mae: 0.0596 - val_mape: 12.2975\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0020 - mae: 0.0344 - mape: 7.1254 - val_loss: 0.0059 - val_mae: 0.0594 - val_mape: 12.4674\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0016 - mae: 0.0313 - mape: 6.4842 - val_loss: 0.0059 - val_mae: 0.0586 - val_mape: 12.3012\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0015 - mae: 0.0295 - mape: 6.1236 - val_loss: 0.0060 - val_mae: 0.0591 - val_mape: 12.3206\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0013 - mae: 0.0276 - mape: 5.7032 - val_loss: 0.0061 - val_mae: 0.0595 - val_mape: 12.3046\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - loss: 0.0012 - mae: 0.0265 - mape: 5.4378 - val_loss: 0.0061 - val_mae: 0.0589 - val_mape: 12.1804\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0062 - mae: 0.0596 - mape: 12.4365\n",
      "no_emotion_or_sentiment:\n",
      "Mean Squared Error: 0.00603902991861105\n",
      "Mean Absolute Error: 0.0591922253370285\n",
      "Mean Absolute Percentage Error: 12.27627944946289\n",
      "Model: no_utterance2\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0612 - mae: 0.1821 - mape: 36.2472 - val_loss: 0.0074 - val_mae: 0.0697 - val_mape: 14.2062\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0086 - mae: 0.0744 - mape: 15.4169 - val_loss: 0.0071 - val_mae: 0.0671 - val_mape: 13.7319\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0070 - mae: 0.0667 - mape: 13.7924 - val_loss: 0.0080 - val_mae: 0.0713 - val_mape: 14.1374\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0055 - mae: 0.0587 - mape: 12.1191 - val_loss: 0.0072 - val_mae: 0.0656 - val_mape: 13.2139\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0041 - mae: 0.0506 - mape: 10.4387 - val_loss: 0.0067 - val_mae: 0.0637 - val_mape: 13.0247\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0030 - mae: 0.0431 - mape: 8.8958 - val_loss: 0.0064 - val_mae: 0.0616 - val_mape: 12.7022\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0024 - mae: 0.0385 - mape: 7.9825 - val_loss: 0.0061 - val_mae: 0.0603 - val_mape: 12.6776\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0020 - mae: 0.0350 - mape: 7.2834 - val_loss: 0.0061 - val_mae: 0.0606 - val_mape: 12.7196\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0019 - mae: 0.0336 - mape: 7.0073 - val_loss: 0.0060 - val_mae: 0.0594 - val_mape: 12.5422\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0016 - mae: 0.0312 - mape: 6.4869 - val_loss: 0.0061 - val_mae: 0.0601 - val_mape: 12.6747\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0061 - mae: 0.0604 - mape: 12.8284\n",
      "no_utterance2:\n",
      "Mean Squared Error: 0.005988821387290955\n",
      "Mean Absolute Error: 0.05992359668016434\n",
      "Mean Absolute Percentage Error: 12.650851249694824\n",
      "Model: only_utterance1\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - loss: 0.0657 - mae: 0.1890 - mape: 37.1809 - val_loss: 0.0069 - val_mae: 0.0665 - val_mape: 13.6478\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0085 - mae: 0.0737 - mape: 15.2890 - val_loss: 0.0064 - val_mae: 0.0630 - val_mape: 13.2570\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0070 - mae: 0.0671 - mape: 14.0814 - val_loss: 0.0064 - val_mae: 0.0627 - val_mape: 13.0773\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0057 - mae: 0.0601 - mape: 12.6525 - val_loss: 0.0063 - val_mae: 0.0625 - val_mape: 13.1098\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0047 - mae: 0.0541 - mape: 11.3686 - val_loss: 0.0062 - val_mae: 0.0612 - val_mape: 12.9863\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0038 - mae: 0.0477 - mape: 10.0214 - val_loss: 0.0062 - val_mae: 0.0612 - val_mape: 13.0999\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0033 - mae: 0.0442 - mape: 9.3313 - val_loss: 0.0062 - val_mae: 0.0607 - val_mape: 12.9062\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0031 - mae: 0.0423 - mape: 8.9214 - val_loss: 0.0062 - val_mae: 0.0608 - val_mape: 13.0506\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0028 - mae: 0.0403 - mape: 8.4751 - val_loss: 0.0064 - val_mae: 0.0614 - val_mape: 13.0848\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0027 - mae: 0.0389 - mape: 8.1846 - val_loss: 0.0063 - val_mae: 0.0611 - val_mape: 13.1042\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.0063 - mae: 0.0613 - mape: 13.2406\n",
      "only_utterance1:\n",
      "Mean Squared Error: 0.006119111552834511\n",
      "Mean Absolute Error: 0.06029250845313072\n",
      "Mean Absolute Percentage Error: 12.923563957214355\n"
     ]
    }
   ],
   "source": [
    "test_losses = {}\n",
    "\n",
    "# Loop through each dataset in training_datasets\n",
    "for dataset_name, (features_train, labels_train, features_test, labels_test) in training_datasets.items():\n",
    "    print(f\"Model: {dataset_name}\")\n",
    "    # Run the model with the specified parameters and get the test loss\n",
    "    test_mse, test_mae, test_mape = run_deep_learning_model_with_params(features_train, labels_train, features_test, labels_test, best_params)\n",
    "    # Save the test loss in the test_losses dictionary with the dataset name as the key\n",
    "    test_losses[dataset_name] = [test_mse, test_mae, test_mape]\n",
    "    print(f\"{dataset_name}:\")\n",
    "    print(f\"Mean Squared Error: {test_mse}\")\n",
    "    print(f\"Mean Absolute Error: {test_mae}\")\n",
    "    print(f\"Mean Absolute Percentage Error: {test_mape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a09d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Losses sorted by Mean Squared Error (sorted):\n",
      "5000features: 0.00571276992559433\n",
      "trigram: 0.005729729309678078\n",
      "quadgram: 0.005730041302740574\n",
      "no_emotion: 0.005732996389269829\n",
      "no_sentiment: 0.005785708781331778\n",
      "bigram: 0.005821918603032827\n",
      "all_data: 0.00585175072774291\n",
      "1000features: 0.0059440056793391705\n",
      "no_utterance2: 0.005988821387290955\n",
      "no_emotion_or_sentiment: 0.00603902991861105\n",
      "only_utterance1: 0.006119111552834511\n",
      "\n",
      "Test Losses sorted by Mean Absolute Error (sorted):\n",
      "trigram: 0.05766170471906662\n",
      "5000features: 0.05788521096110344\n",
      "no_emotion: 0.05790345370769501\n",
      "quadgram: 0.05791456997394562\n",
      "no_sentiment: 0.05831199511885643\n",
      "bigram: 0.05843406170606613\n",
      "all_data: 0.05871236324310303\n",
      "no_emotion_or_sentiment: 0.0591922253370285\n",
      "1000features: 0.059513580054044724\n",
      "no_utterance2: 0.05992359668016434\n",
      "only_utterance1: 0.06029250845313072\n",
      "\n",
      "Test Losses sorted by Mean Absolute Percentage Error (sorted):\n",
      "trigram: 12.133415222167969\n",
      "5000features: 12.178739547729492\n",
      "no_sentiment: 12.225582122802734\n",
      "bigram: 12.250519752502441\n",
      "no_emotion: 12.254773139953613\n",
      "all_data: 12.269471168518066\n",
      "no_emotion_or_sentiment: 12.27627944946289\n",
      "quadgram: 12.371557235717773\n",
      "1000features: 12.483930587768555\n",
      "no_utterance2: 12.650851249694824\n",
      "only_utterance1: 12.923563957214355\n"
     ]
    }
   ],
   "source": [
    "# Print test losses sorted by each metric from least to most loss\n",
    "metrics = [\"Mean Squared Error\", \"Mean Absolute Error\", \"Mean Absolute Percentage Error\"]\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nTest Losses sorted by {metrics[i]} (sorted):\")\n",
    "    for dataset, loss in sorted(test_losses.items(), key=lambda item: item[1][i]):\n",
    "        print(f\"{dataset}: {loss[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2619c7d",
   "metadata": {},
   "source": [
    "### Datasets comparison\n",
    "\n",
    "The test losses, ordered from least loss to most loss with MSE, are as follows:\n",
    "\n",
    "- trigram:                  0.005757459439337254\n",
    "- 5000features:             0.005766735412180424\n",
    "- no_emotion:               0.005812237039208412\n",
    "- quadgram:                 0.0058181206695735455\n",
    "- no_emotion_or_sentiment:  0.005820064339786768\n",
    "- all_data:                 0.0058225891552865505\n",
    "- no_utterance2:            0.005976484622806311\n",
    "- 1000features:             0.006012726575136185\n",
    "- bigram:                   0.006078173406422138\n",
    "- no_sentiment:             0.0061368318274617195\n",
    "- only_utterance1:          0.00617994274944067\n",
    "\n",
    "\n",
    "Let's break these down\n",
    "\n",
    "The baseline is 'all_data' as that is just the raw dataset, so we use this to compare the changes with.\n",
    "\n",
    "#### Limited Vocabulary \n",
    "Limiting it to 5000 features is very roughly limiting it to slightly above half. This did make a noticeable improvement.\n",
    "\n",
    "Limiting the vocabulary to 1000 seemed to make the model worse.\n",
    "\n",
    "This seems to indicate that the least frequent words yields too little information about personality, relative to more frequent words. But you might only want to cut off a certain bottom percentile.\n",
    "\n",
    "We should test with a broader vocabulary and also somewhere between 1000-5000\n",
    "\n",
    "Let's make a test for both 3000 and 7000\n",
    "\n",
    "\n",
    "#### N-Gram\n",
    "Trigram made a noticeable improvement over bigram, quadgram, and unigram.\n",
    "\n",
    "This might indicate that trigram is optimal in capturing the personality of a speaker, as it hits a middleground of splitting up tokens.\n",
    "\n",
    "\n",
    "#### Emotion\n",
    "The emotion tags seems to not make a noticeable difference.\n",
    "\n",
    "\n",
    "#### Sentiment\n",
    "Removing sentiment seems to be detrimental to the model, however, if you remove both emotions and sentiment, then the performance goes back to baseline.\n",
    "\n",
    "We can speculate on why this is the case.\n",
    "\n",
    "There might be information in sentiment that only makes sense to the model together with emotion.\n",
    "\n",
    "\n",
    "#### Utterances\n",
    "It makes intuitive sense that removing utterance 3 (Speaker 1's second utterance) makes the model worse.\n",
    "\n",
    "However, it is interesting that utterance 2 (Speaker 2's utterance) contributes to the models performance when combined with utterance 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f7356",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5cf68cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
