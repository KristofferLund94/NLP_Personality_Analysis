{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9247e3",
   "metadata": {},
   "source": [
    "# Task 6\n",
    "## Create and split training/testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b819709",
   "metadata": {},
   "source": [
    "### Import frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56141cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a31c1663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ee644",
   "metadata": {},
   "source": [
    "### Load Dataset and split labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d446266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dyadic_PELD.tsv', sep='\\t', header=0)\n",
    "\n",
    "\n",
    "labels = df['Personality'].to_numpy()\n",
    "labels = [eval(x) for x in labels]\n",
    "df_labels = pd.DataFrame(labels, columns=['Openness', 'Conscientiousness', 'Extroversion', 'Agreeableness', 'Neuroticism'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd18fb3",
   "metadata": {},
   "source": [
    "### Function for creating training datasets with different variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45c8ccdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_labels(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True):\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(n_gram,n_gram))\n",
    "\n",
    "\n",
    "    # Utterance 1 is always True\n",
    "    utterance_tfidf = tfidf.fit_transform(df['Utterance_1'])\n",
    "    df_features = pd.DataFrame(utterance_tfidf.toarray(), columns=[f\"Utterance1_{word}\" for word in tfidf.get_feature_names_out()])\n",
    "    if emotion:\n",
    "        df_emotions = pd.get_dummies(df[['Emotion_1']])\n",
    "        df_features = pd.concat([df_features, df_emotions], axis=1)\n",
    "    if sentiment:\n",
    "        df_sentiments = pd.get_dummies(df[['Sentiment_1']])\n",
    "        df_features = pd.concat([df_features, df_sentiments], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    if utterance_2:\n",
    "        utterance_tfidf = tfidf.fit_transform(df['Utterance_2'])\n",
    "        df_utterance_tfidf = pd.DataFrame(utterance_tfidf.toarray(), columns=[f\"Utterance2_{word}\" for word in tfidf.get_feature_names_out()])\n",
    "        df_features = pd.concat([df_features, df_utterance_tfidf], axis=1)\n",
    "        if emotion:\n",
    "            df_emotions = pd.get_dummies(df[['Emotion_2']])\n",
    "            df_features = pd.concat([df_features, df_emotions], axis=1)\n",
    "        if sentiment:\n",
    "            df_sentiments = pd.get_dummies(df[['Sentiment_2']])\n",
    "            df_features = pd.concat([df_features, df_sentiments], axis=1)\n",
    "\n",
    "\n",
    "    if utterance_3:\n",
    "        utterance_tfidf = tfidf.fit_transform(df['Utterance_3'])\n",
    "        df_utterance_tfidf = pd.DataFrame(utterance_tfidf.toarray(), columns=[f\"Utterance3_{word}\" for word in tfidf.get_feature_names_out()])\n",
    "        df_features = pd.concat([df_features, df_utterance_tfidf], axis=1)\n",
    "        if emotion:\n",
    "            df_emotions = pd.get_dummies(df[['Emotion_3']])\n",
    "            df_features = pd.concat([df_features, df_emotions], axis=1)\n",
    "        if sentiment:\n",
    "            df_sentiments = pd.get_dummies(df[['Sentiment_3']])\n",
    "            df_features = pd.concat([df_features, df_sentiments], axis=1)\n",
    "\n",
    "    # Train-test split\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(df_features, df_labels, test_size=0.2, random_state=42, stratify=df['Speaker_1'])\n",
    "\n",
    "    return features_train, features_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc183f8",
   "metadata": {},
   "source": [
    "### Setup 6 different ai models to do parameter hypertuning with gridsearch on\n",
    "This is so we can check which models performs best on our dataset, with several different parameters.\n",
    "\n",
    "Note, for now we only use deep learning as this takes a long time to search, it is also unnecesary to hyper optimize for this assignment as the course is about Processing Natural Language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3a3eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deep learning model function\n",
    "def create_deep_learning_model(input_dim, dense_units=512, dropout_rate=0.5):\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_dim,)),  # Define the input layer with the shape\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units // 2, activation='relu'),\n",
    "        Dense(5, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "def grid_search_deep_learning(features_train, labels_train):\n",
    "    model = KerasRegressor(\n",
    "        model=create_deep_learning_model,\n",
    "        input_dim=features_train.shape[1],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Define the param_grid with parameter names directly available in KerasRegressor\n",
    "    param_grid = {\n",
    "        'model__dense_units': [512, 256],\n",
    "        'model__dropout_rate': [0.3, 0.5],\n",
    "        'epochs': [10],\n",
    "        'batch_size': [16, 32]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    print(\"Best Deep Learning Params:\", grid_search.best_params_)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# Linear regression model function\n",
    "def grid_search_linear_regression(features_train, labels_train):\n",
    "    model = LinearRegression()\n",
    "    multi_target_lr = MultiOutputRegressor(model)  # Wrap in MultiOutputRegressor\n",
    "    param_grid = {}\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=multi_target_lr, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    print(\"Best Linear Regression Params:\", grid_search.best_params_)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# Polynomial regression model function\n",
    "def grid_search_polynomial_regression(features_train, labels_train):\n",
    "    model = Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2)),\n",
    "        ('linear', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    multi_target_poly = MultiOutputRegressor(model)  # Wrap in MultiOutputRegressor\n",
    "    param_grid = {\n",
    "        'estimator__poly__degree': [2, 3]  # Adjust the parameter for the pipeline\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=multi_target_poly, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    print(\"Best Polynomial Regression Params:\", grid_search.best_params_)\n",
    "    \n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# SVR model function\n",
    "def grid_search_svr(features_train, labels_train):\n",
    "    # Initialize SVR model\n",
    "    model = SVR()\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'estimator__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'estimator__C': [0.1, 1, 10],\n",
    "        'estimator__epsilon': [0.01, 0.1, 1]\n",
    "    }\n",
    "    \n",
    "    # Use MultiOutputRegressor with GridSearchCV\n",
    "    multi_target_svr = MultiOutputRegressor(model)\n",
    "    grid_search = GridSearchCV(estimator=multi_target_svr, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    \n",
    "    print(\"Best SVR Params:\", grid_search.best_params_)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# Decision tree model function\n",
    "def grid_search_decision_tree(features_train, labels_train):\n",
    "    model = DecisionTreeRegressor()\n",
    "    param_grid = {\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10, 16]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    print(\"Best Decision Tree Params:\", grid_search.best_params_)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "# Random forest model function\n",
    "def grid_search_random_forest(features_train, labels_train):\n",
    "    model = RandomForestRegressor()\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(features_train, labels_train)\n",
    "    print(\"Best Random Forest Params:\", grid_search.best_params_)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_params_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6c8e7",
   "metadata": {},
   "source": [
    "### Run grid search on all models\n",
    "Note, as specified earlier, there is not enough time for this task that doesn't contribute to the field of NLP.\n",
    "\n",
    "Therefore, we comment it out for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9633dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(features_train.shape)\n",
    "\n",
    "# # Linear regression model\n",
    "# best_linear_model, best_params_lr, best_params_lr_repeat, best_score_lr = grid_search_linear_regression(features_train, labels_train)\n",
    "# print(f\"Best Linear Model: {best_linear_model}\")\n",
    "# print(f\"Best Parameters (Linear Regression): {best_params_lr}\")\n",
    "# print(f\"Repeated Best Parameters (Linear Regression): {best_params_lr_repeat}\")\n",
    "# print(f\"Best Score (Linear Regression): {best_score_lr}\")\n",
    "\n",
    "# # Polynomial regression model\n",
    "# best_polynomial_model, best_params_poly, best_params_poly_repeat, best_score_poly = grid_search_polynomial_regression(features_train, labels_train)\n",
    "# print(f\"Best Polynomial Model: {best_polynomial_model}\")\n",
    "# print(f\"Best Parameters (Polynomial Regression): {best_params_poly}\")\n",
    "# print(f\"Repeated Best Parameters (Polynomial Regression): {best_params_poly_repeat}\")\n",
    "# print(f\"Best Score (Polynomial Regression): {best_score_poly}\")\n",
    "\n",
    "# # for dataframe in df_features_array:\n",
    "\n",
    "# # SVR model\n",
    "# # Train-test split\n",
    "# features_train, features_test, labels_train, labels_test = train_test_split(dataframe, df_labels, test_size=0.2, random_state=42, stratify=df['Speaker_1'])\n",
    "# best_svr_model, best_params_svr, best_params_svr_repeat, best_score_svr = grid_search_svr(features_train, labels_train)\n",
    "# print(f\"Best SVR Model: {best_svr_model}\")\n",
    "# print(f\"Best Parameters (SVR): {best_params_svr}\")\n",
    "# print(f\"Repeated Best Parameters (SVR): {best_params_svr_repeat}\")\n",
    "# print(f\"Best Score (SVR): {best_score_svr}\")\n",
    "\n",
    "# # Decision tree model\n",
    "# best_decision_tree_model, best_params_dt, best_params_dt_repeat, best_score_dt = grid_search_decision_tree(features_train, labels_train)\n",
    "# print(f\"Best Decision Tree Model: {best_decision_tree_model}\")\n",
    "# print(f\"Best Parameters (Decision Tree): {best_params_dt}\")\n",
    "# print(f\"Repeated Best Parameters (Decision Tree): {best_params_dt_repeat}\")\n",
    "# print(f\"Best Score (Decision Tree): {best_score_dt}\")\n",
    "\n",
    "# # Random forest model\n",
    "# best_random_forest_model, best_params_rf, best_params_rf_repeat, best_score_rf = grid_search_random_forest(features_train, labels_train)\n",
    "# print(f\"Best Random Forest Model: {best_random_forest_model}\")\n",
    "# print(f\"Best Parameters (Random Forest): {best_params_rf}\")\n",
    "# print(f\"Repeated Best Parameters (Random Forest): {best_params_rf_repeat}\")\n",
    "# print(f\"Best Score (Random Forest): {best_score_rf}\")\n",
    "\n",
    "# # Deep learning model\n",
    "# best_deep_learning_model, best_params_dl, best_params_dl_repeat, best_score_dl = grid_search_deep_learning(features_train, labels_train)\n",
    "# print(f\"Best Deep Learning Model: {best_deep_learning_model}\")\n",
    "# print(f\"Best Parameters (Deep Learning): {best_params_dl}\")\n",
    "# print(f\"Repeated Best Parameters (Deep Learning): {best_params_dl_repeat}\")\n",
    "# print(f\"Best Score (Deep Learning): {best_score_dl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb9e97",
   "metadata": {},
   "source": [
    "### Run grid search only one a deep learning model\n",
    "Therefore we only run grid search on deep learning, and not all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f597fc4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m features_train, features_test, labels_train, labels_test \u001b[38;5;241m=\u001b[39m get_training_labels()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Deep learning model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m best_deep_learning_model, best_params_dl, best_params_dl_repeat, best_score_dl \u001b[38;5;241m=\u001b[39m grid_search_deep_learning(features_train, labels_train)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Deep Learning Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_deep_learning_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters (Deep Learning): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params_dl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 30\u001b[0m, in \u001b[0;36mgrid_search_deep_learning\u001b[1;34m(features_train, labels_train)\u001b[0m\n\u001b[0;32m     22\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel__dense_units\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m256\u001b[39m],\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel__dropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.5\u001b[39m],\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10\u001b[39m],\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m]\n\u001b[0;32m     27\u001b[0m }\n\u001b[0;32m     29\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(features_train, labels_train)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Deep Learning Params:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_, grid_search\u001b[38;5;241m.\u001b[39mbest_params_, grid_search\u001b[38;5;241m.\u001b[39mbest_params_, grid_search\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    918\u001b[0m         clone(base_estimator),\n\u001b[0;32m    919\u001b[0m         X,\n\u001b[0;32m    920\u001b[0m         y,\n\u001b[0;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    931\u001b[0m     )\n\u001b[0;32m    932\u001b[0m )\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:895\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    893\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 895\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    899\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py:770\u001b[0m, in \u001b[0;36mBaseWrapper.fit\u001b[1;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    765\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit__epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[0;32m    767\u001b[0m )\n\u001b[0;32m    768\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 770\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    771\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    772\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    773\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    774\u001b[0m     warm_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start,\n\u001b[0;32m    775\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    776\u001b[0m )\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py:938\u001b[0m, in \u001b[0;36mBaseWrapper._fit\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    934\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_encoder_\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_model_compatibility(y)\n\u001b[1;32m--> 938\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_keras_model(\n\u001b[0;32m    939\u001b[0m     X,\n\u001b[0;32m    940\u001b[0m     y,\n\u001b[0;32m    941\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    942\u001b[0m     warm_start\u001b[38;5;241m=\u001b[39mwarm_start,\n\u001b[0;32m    943\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[0;32m    944\u001b[0m     initial_epoch\u001b[38;5;241m=\u001b[39minitial_epoch,\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    946\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\scikeras\\wrappers.py:535\u001b[0m, in \u001b[0;36mBaseWrapper._fit_keras_model\u001b[1;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m         hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_args)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     hist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39mfit(x\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_args)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warm_start \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhistory_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m initial_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory_ \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1684\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1685\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1686\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1687\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1688\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1689\u001b[0m   )\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features_train, features_test, labels_train, labels_test = get_training_labels()\n",
    "\n",
    "\n",
    "# Deep learning model\n",
    "best_deep_learning_model, best_params_dl, best_params_dl_repeat, best_score_dl = grid_search_deep_learning(features_train, labels_train)\n",
    "print(f\"Best Deep Learning Model: {best_deep_learning_model}\")\n",
    "print(f\"Best Parameters (Deep Learning): {best_params_dl}\")\n",
    "print(f\"Repeated Best Parameters (Deep Learning): {best_params_dl_repeat}\")\n",
    "print(f\"Best Score (Deep Learning): {best_score_dl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a9f81f",
   "metadata": {},
   "source": [
    "The best deep learning parameters for this problem are these:\n",
    "\n",
    "batch_size: 32\n",
    "\n",
    "epochs: 10\n",
    "\n",
    "model__dense_units: 256\n",
    "\n",
    "model__dropout_rate: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6f6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'batch_size': 32, 'epochs': 10, 'model__dense_units': 256, 'model__dropout_rate': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc17a93",
   "metadata": {},
   "source": [
    "### Create a list of datasets that differs in simple terms\n",
    "This is the meat of this task. We want to determine how different features affect the model.\n",
    "\n",
    "Therefore we start by making only one or two changes in each dataset in order to be able to measure the impact of every single feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a6ebbdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m training_datasets \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5000features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1000features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigram\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigram\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquadgram\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_emotion_or_sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_utterance2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_utterance1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m     17\u001b[0m }\n",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m, in \u001b[0;36mget_training_labels\u001b[1;34m(max_features, n_gram, emotion, sentiment, utterance_2, utterance_3)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m emotion:\n\u001b[0;32m     22\u001b[0m     df_emotions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotion_2\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m---> 23\u001b[0m     df_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_features, df_emotions], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentiment:\n\u001b[0;32m     25\u001b[0m     df_sentiments \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment_2\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    686\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconcat_horizontal(mgrs, axes)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[0m, in \u001b[0;36m_maybe_reindex_columns_na_proxy\u001b[1;34m(axes, mgrs_indexers, needs_copy)\u001b[0m\n\u001b[0;32m    220\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    221\u001b[0m             axes[i],\n\u001b[0;32m    222\u001b[0m             indexers[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m             use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[1;32m--> 230\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    232\u001b[0m     new_mgrs\u001b[38;5;241m.\u001b[39mappend(mgr)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    601\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 604\u001b[0m     res\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1788\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m _consolidate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2269\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2267\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2269\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m _merge_blocks(\n\u001b[0;32m   2270\u001b[0m         \u001b[38;5;28mlist\u001b[39m(group_blocks), dtype\u001b[38;5;241m=\u001b[39mdtype, can_consolidate\u001b[38;5;241m=\u001b[39m_can_consolidate\n\u001b[0;32m   2271\u001b[0m     )\n\u001b[0;32m   2272\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2304\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m new_values[argsort]\n\u001b[0;32m   2302\u001b[0m     new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[1;32m-> 2304\u001b[0m     bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n\u001b[0;32m   2305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [new_block_2d(new_values, placement\u001b[38;5;241m=\u001b[39mbp)], \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;66;03m# can't consolidate --> no merge\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_datasets = {\n",
    "    \"all_data\": list(get_training_labels(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"5000features\": list(get_training_labels(max_features=5000, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"1000features\": list(get_training_labels(max_features=1000, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"bigram\": list(get_training_labels(max_features=None, n_gram=2, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"trigram\": list(get_training_labels(max_features=None, n_gram=3, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"quadgram\": list(get_training_labels(max_features=None, n_gram=4, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"no_sentiment\": list(get_training_labels(max_features=None, n_gram=1, emotion=True, sentiment=False, utterance_2=True, utterance_3=True)),\n",
    "    \"no_emotion\": list(get_training_labels(max_features=None, n_gram=1, emotion=False, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"no_emotion_or_sentiment\": list(get_training_labels(max_features=None, n_gram=1, emotion=False, sentiment=False, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"no_utterance2\": list(get_training_labels(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=False, utterance_3=True)),\n",
    "    \"only_utterance1\": list(get_training_labels(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=False, utterance_3=False))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6931b6",
   "metadata": {},
   "source": [
    "### Create a function for running the model on all datasets\n",
    "We split the 'train' into 'train/val', so that we have training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "386103ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_deep_learning_model_with_params(features_train, features_test, labels_train, labels_test, params):\n",
    "    # Extract parameters from the dictionary\n",
    "    dense_units = params.get('model__dense_units', 512)\n",
    "    dropout_rate = params.get('model__dropout_rate', 0.5)\n",
    "    epochs = params.get('epochs', 10)\n",
    "    batch_size = params.get('batch_size', 32)\n",
    "    \n",
    "    # Train-val split\n",
    "    df_full_train, df_full_test, labels_train, labels_test = train_test_split(df, df_labels, test_size=0.2, random_state=42, stratify=df['Speaker_1'])\n",
    "    features_inner_train, features_val, labels_inner_train, labels_val = train_test_split(features_train, labels_train, test_size=0.2, random_state=42, stratify=df_full_train['Speaker_1'])\n",
    "\n",
    "    # Define the model architecture based on chosen parameters\n",
    "    model = Sequential([\n",
    "        Input(shape=(features_train.shape[1],)),\n",
    "        Dense(dense_units, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(dense_units // 2, activation='relu'),\n",
    "        Dense(5, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(features_inner_train, labels_inner_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(features_val, labels_val),\n",
    "                        verbose=1)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = model.evaluate(features_test, labels_test)\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f761ce",
   "metadata": {},
   "source": [
    "### Run the model on every dataset to compare test loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7648de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: all_data\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - loss: 0.0581 - val_loss: 0.0069\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0089 - val_loss: 0.0071\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0052 - val_loss: 0.0076\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0037 - val_loss: 0.0063\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0026 - val_loss: 0.0058\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.0057\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0018 - val_loss: 0.0058\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.0058\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0014 - val_loss: 0.0059\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0061\n",
      "Test Loss: 0.0058225891552865505\n",
      "all_data: 0.0058225891552865505\n",
      "Model: 5000features\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 0.0484 - val_loss: 0.0073\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0046 - val_loss: 0.0071\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0034 - val_loss: 0.0071\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0026 - val_loss: 0.0059\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0020 - val_loss: 0.0059\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0017 - val_loss: 0.0058\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - loss: 0.0015 - val_loss: 0.0058\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 0.0058\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0059\n",
      "Test Loss: 0.005766735412180424\n",
      "5000features: 0.005766735412180424\n",
      "Model: 1000features\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - loss: 0.0496 - val_loss: 0.0072\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0091 - val_loss: 0.0072\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 0.0072 - val_loss: 0.0080\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0060 - val_loss: 0.0067\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0046 - val_loss: 0.0065\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0035 - val_loss: 0.0063\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0027 - val_loss: 0.0062\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0023 - val_loss: 0.0060\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0020 - val_loss: 0.0060\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 0.0018 - val_loss: 0.0060\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0061\n",
      "Test Loss: 0.006012726575136185\n",
      "1000features: 0.006012726575136185\n",
      "Model: bigram\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 141ms/step - loss: 0.0526 - val_loss: 0.0085\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - loss: 0.0124 - val_loss: 0.0073\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 131ms/step - loss: 0.0066 - val_loss: 0.0100\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - loss: 0.0040 - val_loss: 0.0072\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 136ms/step - loss: 0.0027 - val_loss: 0.0067\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 134ms/step - loss: 0.0021 - val_loss: 0.0061\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 132ms/step - loss: 0.0016 - val_loss: 0.0060\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 134ms/step - loss: 0.0014 - val_loss: 0.0060\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 135ms/step - loss: 0.0012 - val_loss: 0.0058\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 128ms/step - loss: 0.0011 - val_loss: 0.0059\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0062\n",
      "Test Loss: 0.006078173406422138\n",
      "bigram: 0.006078173406422138\n",
      "Model: trigram\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 199ms/step - loss: 0.0595 - val_loss: 0.0067\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 175ms/step - loss: 0.0135 - val_loss: 0.0092\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 175ms/step - loss: 0.0077 - val_loss: 0.0084\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 174ms/step - loss: 0.0048 - val_loss: 0.0083\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 179ms/step - loss: 0.0035 - val_loss: 0.0068\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 180ms/step - loss: 0.0025 - val_loss: 0.0060\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 188ms/step - loss: 0.0021 - val_loss: 0.0060\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 178ms/step - loss: 0.0018 - val_loss: 0.0057\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 176ms/step - loss: 0.0015 - val_loss: 0.0057\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 173ms/step - loss: 0.0014 - val_loss: 0.0056\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0059\n",
      "Test Loss: 0.005757459439337254\n",
      "trigram: 0.005757459439337254\n",
      "Model: quadgram\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 175ms/step - loss: 0.0514 - val_loss: 0.0069\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - loss: 0.0141 - val_loss: 0.0098\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - loss: 0.0076 - val_loss: 0.0115\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 170ms/step - loss: 0.0050 - val_loss: 0.0098\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 168ms/step - loss: 0.0038 - val_loss: 0.0075\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - loss: 0.0029 - val_loss: 0.0069\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - loss: 0.0023 - val_loss: 0.0059\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 165ms/step - loss: 0.0019 - val_loss: 0.0058\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 167ms/step - loss: 0.0017 - val_loss: 0.0056\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 171ms/step - loss: 0.0015 - val_loss: 0.0057\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0059\n",
      "Test Loss: 0.0058181206695735455\n",
      "quadgram: 0.0058181206695735455\n",
      "Model: no_sentiment\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - loss: 0.0535 - val_loss: 0.0072\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0086 - val_loss: 0.0072\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0045 - val_loss: 0.0075\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0033 - val_loss: 0.0066\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0024 - val_loss: 0.0062\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0020 - val_loss: 0.0063\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0017 - val_loss: 0.0060\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0014 - val_loss: 0.0060\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0013 - val_loss: 0.0062\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0063\n",
      "Test Loss: 0.0061368318274617195\n",
      "no_sentiment: 0.0061368318274617195\n",
      "Model: no_emotion\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - loss: 0.0521 - val_loss: 0.0078\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0089 - val_loss: 0.0073\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0061 - val_loss: 0.0066\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0042 - val_loss: 0.0066\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0029 - val_loss: 0.0060\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0021 - val_loss: 0.0059\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0018 - val_loss: 0.0060\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0015 - val_loss: 0.0058\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0014 - val_loss: 0.0059\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0012 - val_loss: 0.0059\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0060\n",
      "Test Loss: 0.005812237039208412\n",
      "no_emotion: 0.005812237039208412\n",
      "Model: no_emotion_or_sentiment\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - loss: 0.0738 - val_loss: 0.0082\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0091 - val_loss: 0.0073\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0061 - val_loss: 0.0076\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0038 - val_loss: 0.0065\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0027 - val_loss: 0.0064\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0020 - val_loss: 0.0060\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0017 - val_loss: 0.0059\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0015 - val_loss: 0.0059\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.0013 - val_loss: 0.0060\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0011 - val_loss: 0.0059\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0061\n",
      "Test Loss: 0.005820064339786768\n",
      "no_emotion_or_sentiment: 0.005820064339786768\n",
      "Model: no_utterance2\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 22ms/step - loss: 0.0658 - val_loss: 0.0067\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0087 - val_loss: 0.0081\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0071 - val_loss: 0.0064\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0054 - val_loss: 0.0064\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0041 - val_loss: 0.0063\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0031 - val_loss: 0.0061\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0025 - val_loss: 0.0060\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0021 - val_loss: 0.0061\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 0.0019 - val_loss: 0.0060\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0017 - val_loss: 0.0060\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0061\n",
      "Test Loss: 0.005976484622806311\n",
      "no_utterance2: 0.005976484622806311\n",
      "Model: only_utterance1\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.0594 - val_loss: 0.0069\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0085 - val_loss: 0.0064\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0072 - val_loss: 0.0066\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0060 - val_loss: 0.0073\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0050 - val_loss: 0.0062\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0039 - val_loss: 0.0062\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0034 - val_loss: 0.0063\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0031 - val_loss: 0.0061\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0028 - val_loss: 0.0062\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 0.0026 - val_loss: 0.0062\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0064\n",
      "Test Loss: 0.00617994274944067\n",
      "only_utterance1: 0.00617994274944067\n"
     ]
    }
   ],
   "source": [
    "test_losses = {}\n",
    "\n",
    "# Loop through each dataset in training_datasets\n",
    "for dataset_name, (features_train, labels_train, features_test, labels_test) in training_datasets.items():\n",
    "    print(f\"Model: {dataset_name}\")\n",
    "    # Run the model with the specified parameters and get the test loss\n",
    "    test_loss = run_deep_learning_model_with_params(features_train, labels_train, features_test, labels_test, best_params)\n",
    "    # Save the test loss in the test_losses dictionary with the dataset name as the key\n",
    "    test_losses[dataset_name] = test_loss\n",
    "    print(f\"{dataset_name}: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a09d975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Losses (sorted):\n",
      "trigram: 0.005757459439337254\n",
      "5000features: 0.005766735412180424\n",
      "no_emotion: 0.005812237039208412\n",
      "quadgram: 0.0058181206695735455\n",
      "no_emotion_or_sentiment: 0.005820064339786768\n",
      "all_data: 0.0058225891552865505\n",
      "no_utterance2: 0.005976484622806311\n",
      "1000features: 0.006012726575136185\n",
      "bigram: 0.006078173406422138\n",
      "no_sentiment: 0.0061368318274617195\n",
      "only_utterance1: 0.00617994274944067\n"
     ]
    }
   ],
   "source": [
    "# Print all test losses sorted from least to most loss\n",
    "print(\"Test Losses (sorted):\")\n",
    "for dataset, loss in sorted(test_losses.items(), key=lambda item: item[1]):\n",
    "    print(f\"{dataset}: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2619c7d",
   "metadata": {},
   "source": [
    "### Datasets comparison\n",
    "\n",
    "The test losses, ordered from least loss to most loss, are as follows:\n",
    "\n",
    "- trigram:                  0.005757459439337254\n",
    "- 5000features:             0.005766735412180424\n",
    "- no_emotion:               0.005812237039208412\n",
    "- quadgram:                 0.0058181206695735455\n",
    "- no_emotion_or_sentiment:  0.005820064339786768\n",
    "- all_data:                 0.0058225891552865505\n",
    "- no_utterance2:            0.005976484622806311\n",
    "- 1000features:             0.006012726575136185\n",
    "- bigram:                   0.006078173406422138\n",
    "- no_sentiment:             0.0061368318274617195\n",
    "- only_utterance1:          0.00617994274944067\n",
    "\n",
    "\n",
    "Let's break these down\n",
    "\n",
    "The baseline is 'all_data' as that is just the raw dataset, so we use this to compare the changes with.\n",
    "\n",
    "#### Limited Vocabulary \n",
    "Limiting it to 5000 features is very roughly limiting it to slightly above half. This did make a noticeable improvement.\n",
    "\n",
    "Limiting the vocabulary to 1000 seemed to make the model worse.\n",
    "\n",
    "This seems to indicate that the least frequent words yields too little information about personality, relative to more frequent words. But you might only want to cut off a certain bottom percentile.\n",
    "\n",
    "We should test with a broader vocabulary and also somewhere between 1000-5000\n",
    "\n",
    "Let's make a test for both 3000 and 7000\n",
    "\n",
    "\n",
    "#### N-Gram\n",
    "trigram made a noticeable improvement over bigram, quadgram, and unigram.\n",
    "\n",
    "This might indicate that trigram is optimal in capturing the personality of a speaker, as it hits a middleground of splitting up tokens.\n",
    "\n",
    "\n",
    "#### Emotion\n",
    "The emotion tags seems to not make a noticeable difference.\n",
    "\n",
    "\n",
    "#### Sentiment\n",
    "Removing sentiment seems to be detrimental to the model, however, if you remove both emotions and sentiment, then the performance goes back to baseline.\n",
    "\n",
    "We can speculate on why this is the case.\n",
    "\n",
    "There might be information in sentiment that only makes sense to the model together with emotion.\n",
    "\n",
    "\n",
    "#### Utterances\n",
    "It makes intuitive sense that removing utterance 3 (Speaker 1's second utternace) makes the model worse.\n",
    "\n",
    "However, it is interesting that utterance 2 (Speaker 2's utternace) contributes to the models performance when combined with utterance 3\n",
    "\n",
    "\n",
    "### Further testing\n",
    "\n",
    "We should test some of these features more, and also a combination of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adbef496",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m training_datasets_2 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7000features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3000features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigram_5000features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigram_5000features_noEmotion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigram_5000features_noEmotion_noSentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      8\u001b[0m }\n",
      "Cell \u001b[1;32mIn[11], line 23\u001b[0m, in \u001b[0;36mget_training_labels\u001b[1;34m(max_features, n_gram, emotion, sentiment, utterance_2, utterance_3)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m emotion:\n\u001b[0;32m     22\u001b[0m     df_emotions \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotion_2\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m---> 23\u001b[0m     df_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_features, df_emotions], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sentiment:\n\u001b[0;32m     25\u001b[0m     df_sentiments \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentiment_2\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[0;32m    383\u001b[0m     objs,\n\u001b[0;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m    393\u001b[0m )\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m concatenate_managers(\n\u001b[0;32m    685\u001b[0m     mgrs_indexers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnew_axes, concat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm_axis, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[0;32m    686\u001b[0m )\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:131\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Assertions disabled for performance\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# for tup in mgrs_indexers:\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m#    # caller is responsible for ensuring this\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m#    indexers = tup[1]\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m#    assert concat_axis not in indexers\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concat_axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 131\u001b[0m     mgrs \u001b[38;5;241m=\u001b[39m _maybe_reindex_columns_na_proxy(axes, mgrs_indexers, needs_copy)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mgrs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconcat_horizontal(mgrs, axes)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mgrs_indexers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mgrs_indexers[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnblocks \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\concat.py:230\u001b[0m, in \u001b[0;36m_maybe_reindex_columns_na_proxy\u001b[1;34m(axes, mgrs_indexers, needs_copy)\u001b[0m\n\u001b[0;32m    220\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    221\u001b[0m             axes[i],\n\u001b[0;32m    222\u001b[0m             indexers[i],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m             use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# only relevant for i==0\u001b[39;00m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m needs_copy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexers:\n\u001b[1;32m--> 230\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m mgr\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    232\u001b[0m     new_mgrs\u001b[38;5;241m.\u001b[39mappend(mgr)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_mgrs\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:604\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    601\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 604\u001b[0m     res\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1791\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rebuild_blknos_and_blklocs()\n",
      "File \u001b[1;32minternals.pyx:755\u001b[0m, in \u001b[0;36mpandas._libs.internals.BlockManager._rebuild_blknos_and_blklocs\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\base.py:84\u001b[0m, in \u001b[0;36mDataManager.shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Shape:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ax) \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32mc:\\Users\\Krist\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\base.py:84\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshape\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Shape:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ax) \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_datasets_2 = {\n",
    "    \"7000features\": list(get_training_labels(max_features=7000, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"3000features\": list(get_training_labels(max_features=3000, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"trigram_5000features\": list(get_training_labels(max_features=5000, n_gram=3, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"trigram_5000features_noEmotion\": list(get_training_labels(max_features=5000, n_gram=3, emotion=False, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"trigram_5000features_noEmotion_noSentiment\": list(get_training_labels(max_features=5000, n_gram=3, emotion=False, sentiment=False, utterance_2=True, utterance_3=True)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0425b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 7000features\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - loss: 0.0587 - val_loss: 0.0069\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0091 - val_loss: 0.0075\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0051 - val_loss: 0.0075\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - loss: 0.0039 - val_loss: 0.0067\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0028 - val_loss: 0.0061\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 0.0059\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0017 - val_loss: 0.0058\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - loss: 0.0016 - val_loss: 0.0059\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - loss: 0.0013 - val_loss: 0.0059\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0061\n",
      "Test Loss: 0.005921616684645414\n",
      "7000features: 0.005921616684645414\n",
      "Model: 3000features\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0499 - val_loss: 0.0069\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0089 - val_loss: 0.0081\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0047 - val_loss: 0.0063\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - loss: 0.0034 - val_loss: 0.0063\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.0025 - val_loss: 0.0061\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0020 - val_loss: 0.0058\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 19ms/step - loss: 0.0018 - val_loss: 0.0060\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 20ms/step - loss: 0.0015 - val_loss: 0.0059\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.0014 - val_loss: 0.0057\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0060\n",
      "Test Loss: 0.005785919725894928\n",
      "3000features: 0.005785919725894928\n",
      "Model: trigram_5000features\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 0.0577 - val_loss: 0.0076\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - loss: 0.0094 - val_loss: 0.0073\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0053 - val_loss: 0.0076\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0041 - val_loss: 0.0062\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0031 - val_loss: 0.0061\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0026 - val_loss: 0.0059\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0023 - val_loss: 0.0060\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.0021 - val_loss: 0.0059\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0019 - val_loss: 0.0059\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0063\n",
      "Test Loss: 0.006059326231479645\n",
      "trigram_5000features: 0.006059326231479645\n",
      "Model: trigram_5000features_noEmotion\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 32ms/step - loss: 0.0633 - val_loss: 0.0076\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0102 - val_loss: 0.0072\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0072 - val_loss: 0.0063\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0046 - val_loss: 0.0063\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0035 - val_loss: 0.0060\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 0.0027 - val_loss: 0.0058\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0024 - val_loss: 0.0060\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 0.0021 - val_loss: 0.0062\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0020 - val_loss: 0.0060\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 0.0018 - val_loss: 0.0060\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0064\n",
      "Test Loss: 0.006188692059367895\n",
      "trigram_5000features_noEmotion: 0.006188692059367895\n",
      "Model: trigram_5000features_noEmotion_noSentiment\n",
      "Epoch 1/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - loss: 0.0882 - val_loss: 0.0101\n",
      "Epoch 2/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0114 - val_loss: 0.0069\n",
      "Epoch 3/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0059 - val_loss: 0.0064\n",
      "Epoch 4/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 32ms/step - loss: 0.0038 - val_loss: 0.0062\n",
      "Epoch 5/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0028 - val_loss: 0.0060\n",
      "Epoch 6/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0024 - val_loss: 0.0061\n",
      "Epoch 7/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0022 - val_loss: 0.0062\n",
      "Epoch 8/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 34ms/step - loss: 0.0019 - val_loss: 0.0061\n",
      "Epoch 9/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 0.0018 - val_loss: 0.0061\n",
      "Epoch 10/10\n",
      "\u001b[1m131/131\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - loss: 0.0017 - val_loss: 0.0061\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0066\n",
      "Test Loss: 0.006352831143885851\n",
      "trigram_5000features_noEmotion_noSentiment: 0.006352831143885851\n"
     ]
    }
   ],
   "source": [
    "test_losses_2 = {}\n",
    "\n",
    "# Loop through each dataset in training_datasets\n",
    "for dataset_name, (features_train, labels_train, features_test, labels_test) in training_datasets_2.items():\n",
    "    print(f\"Model: {dataset_name}\")\n",
    "    # Run the model with the specified parameters and get the test loss\n",
    "    test_loss = run_deep_learning_model_with_params(features_train, labels_train, features_test, labels_test, best_params)\n",
    "    # Save the test loss in the test_losses dictionary with the dataset name as the key\n",
    "    test_losses_2[dataset_name] = test_loss\n",
    "    print(f\"{dataset_name}: {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c68d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Losses for previous test (sorted):\n",
      "trigram: 0.005757459439337254\n",
      "5000features: 0.005766735412180424\n",
      "no_emotion: 0.005812237039208412\n",
      "quadgram: 0.0058181206695735455\n",
      "no_emotion_or_sentiment: 0.005820064339786768\n",
      "all_data: 0.0058225891552865505\n",
      "no_utterance2: 0.005976484622806311\n",
      "1000features: 0.006012726575136185\n",
      "bigram: 0.006078173406422138\n",
      "no_sentiment: 0.0061368318274617195\n",
      "only_utterance1: 0.00617994274944067\n",
      "\n",
      "Test Losses for this test (sorted):\n",
      "3000features: 0.005785919725894928\n",
      "7000features: 0.005921616684645414\n",
      "trigram_5000features: 0.006059326231479645\n",
      "trigram_5000features_noEmotion: 0.006188692059367895\n",
      "trigram_5000features_noEmotion_noSentiment: 0.006352831143885851\n"
     ]
    }
   ],
   "source": [
    "# Print all test losses for the previous test sorted from least to most loss\n",
    "print(\"Test Losses for previous test (sorted):\")\n",
    "for dataset, loss in sorted(test_losses.items(), key=lambda item: item[1]):\n",
    "    print(f\"{dataset}: {loss}\")\n",
    "\n",
    "# Print all test losses for this test sorted from least to most loss\n",
    "print(\"\\nTest Losses for this test (sorted):\")\n",
    "for dataset, loss in sorted(test_losses_2.items(), key=lambda item: item[1]):\n",
    "    print(f\"{dataset}: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded7bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Test Losses (sorted):\n",
      "trigram: 0.005757459439337254\n",
      "5000features: 0.005766735412180424\n",
      "* 3000features: 0.005785919725894928\n",
      "no_emotion: 0.005812237039208412\n",
      "quadgram: 0.0058181206695735455\n",
      "no_emotion_or_sentiment: 0.005820064339786768\n",
      "all_data: 0.0058225891552865505\n",
      "* 7000features: 0.005921616684645414\n",
      "no_utterance2: 0.005976484622806311\n",
      "1000features: 0.006012726575136185\n",
      "* trigram_5000features: 0.006059326231479645\n",
      "bigram: 0.006078173406422138\n",
      "no_sentiment: 0.0061368318274617195\n",
      "only_utterance1: 0.00617994274944067\n",
      "* trigram_5000features_noEmotion: 0.006188692059367895\n",
      "* trigram_5000features_noEmotion_noSentiment: 0.006352831143885851\n"
     ]
    }
   ],
   "source": [
    "# Combine the two dictionaries into one list with a flag for highlighting\n",
    "combined_losses = [(dataset, loss, 'second' if dataset in test_losses_2 else 'first') for dataset, loss in test_losses.items()]\n",
    "combined_losses.extend([(dataset, loss, 'second') for dataset, loss in test_losses_2.items()])\n",
    "\n",
    "# Sort the combined list by the loss value\n",
    "combined_losses.sort(key=lambda item: item[1])\n",
    "\n",
    "# Print the sorted losses with highlighting\n",
    "print(\"Combined Test Losses (sorted):\")\n",
    "for dataset, loss, source in combined_losses:\n",
    "    if source == 'second':\n",
    "        print(f\"* {dataset}: {loss}\")  # Highlight the dataset from the second test\n",
    "    else:\n",
    "        print(f\"{dataset}: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb17d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d05a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_labels_pca(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True):\n",
    "    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(n_gram, n_gram))\n",
    "\n",
    "    # Utterance 1 is always True\n",
    "    utterance_tfidf = tfidf.fit_transform(df['Utterance_1'])\n",
    "    df_features = pd.DataFrame(utterance_tfidf.toarray(), columns=[f\"Utterance1_{word}\" for word in tfidf.get_feature_names_out()])\n",
    "    \n",
    "    if emotion:\n",
    "        df_emotions = pd.get_dummies(df[['Emotion_1']])\n",
    "        df_features = pd.concat([df_features, df_emotions], axis=1)\n",
    "    if sentiment:\n",
    "        df_sentiments = pd.get_dummies(df[['Sentiment_1']])\n",
    "        df_features = pd.concat([df_features, df_sentiments], axis=1)\n",
    "\n",
    "    if utterance_2:\n",
    "        utterance_tfidf = tfidf.fit_transform(df['Utterance_2'])\n",
    "        df_utterance_tfidf = pd.DataFrame(utterance_tfidf.toarray(), columns=[f\"Utterance2_{word}\" for word in tfidf.get_feature_names_out()])\n",
    "        df_features = pd.concat([df_features, df_utterance_tfidf], axis=1)\n",
    "        \n",
    "        if emotion:\n",
    "            df_emotions = pd.get_dummies(df[['Emotion_2']])\n",
    "            df_features = pd.concat([df_features, df_emotions], axis=1)\n",
    "        if sentiment:\n",
    "            df_sentiments = pd.get_dummies(df[['Sentiment_2']])\n",
    "            df_features = pd.concat([df_features, df_sentiments], axis=1)\n",
    "\n",
    "    if utterance_3:\n",
    "        utterance_tfidf = tfidf.fit_transform(df['Utterance_3'])\n",
    "        df_utterance_tfidf = pd.DataFrame(utterance_tfidf.toarray(), columns=[f\"Utterance3_{word}\" for word in tfidf.get_feature_names_out()])\n",
    "        df_features = pd.concat([df_features, df_utterance_tfidf], axis=1)\n",
    "        \n",
    "        if emotion:\n",
    "            df_emotions = pd.get_dummies(df[['Emotion_3']])\n",
    "            df_features = pd.concat([df_features, df_emotions], axis=1)\n",
    "        if sentiment:\n",
    "            df_sentiments = pd.get_dummies(df[['Sentiment_3']])\n",
    "            df_features = pd.concat([df_features, df_sentiments], axis=1)\n",
    "\n",
    "    # Assuming df_labels is defined elsewhere in your code\n",
    "    df_labels = df['Speaker_1']  # Adjust according to your dataset\n",
    "\n",
    "    # Train-test split\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(df_features, df_labels, test_size=0.2, random_state=42, stratify=df['Speaker_1'])\n",
    "\n",
    "    # Standardize the features before applying PCA\n",
    "    scaler = StandardScaler()\n",
    "    features_train_scaled = scaler.fit_transform(features_train)\n",
    "    features_test_scaled = scaler.transform(features_test)\n",
    "\n",
    "    # Apply PCA to keep 90% of the variance\n",
    "    pca = PCA(n_components=0.10)\n",
    "    features_train_pca = pca.fit_transform(features_train_scaled)\n",
    "    features_test_pca = pca.transform(features_test_scaled)\n",
    "\n",
    "    return features_train_pca, features_test_pca, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cb54473",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m training_datasets_pca \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5000features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1000features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigram\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigram\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquadgram\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_emotion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_emotion_or_sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_utterance2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_utterance1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m     17\u001b[0m }\n\u001b[0;32m     20\u001b[0m training_datasets_2_pca \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7000features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3000features\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigram_5000features_noEmotion_noSentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(get_training_labels_pca(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     27\u001b[0m }\n",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m, in \u001b[0;36mget_training_labels_pca\u001b[1;34m(max_features, n_gram, emotion, sentiment, utterance_2, utterance_3)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_training_labels_pca\u001b[39m(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_gram\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, emotion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sentiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, utterance_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     tfidf \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39mmax_features, ngram_range\u001b[38;5;241m=\u001b[39m(n_gram, n_gram))\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Utterance 1 is always True\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     utterance_tfidf \u001b[38;5;241m=\u001b[39m tfidf\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUtterance_1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "training_datasets_pca = {\n",
    "    \"all_data\": list(get_training_labels_pca(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"5000features\": list(get_training_labels_pca(max_features=5000, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"1000features\": list(get_training_labels_pca(max_features=1000, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"bigram\": list(get_training_labels_pca(max_features=None, n_gram=2, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"trigram\": list(get_training_labels_pca(max_features=None, n_gram=3, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"quadgram\": list(get_training_labels_pca(max_features=None, n_gram=4, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"no_sentiment\": list(get_training_labels_pca(max_features=None, n_gram=1, emotion=True, sentiment=False, utterance_2=True, utterance_3=True)),\n",
    "    \"no_emotion\": list(get_training_labels_pca(max_features=None, n_gram=1, emotion=False, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"no_emotion_or_sentiment\": list(get_training_labels_pca(max_features=None, n_gram=1, emotion=False, sentiment=False, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"no_utterance2\": list(get_training_labels_pca(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=False, utterance_3=True)),\n",
    "    \"only_utterance1\": list(get_training_labels_pca(max_features=None, n_gram=1, emotion=True, sentiment=True, utterance_2=False, utterance_3=False))\n",
    "}\n",
    "\n",
    "\n",
    "training_datasets_2_pca = {\n",
    "    \"7000features\": list(get_training_labels_pca(max_features=7000, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"3000features\": list(get_training_labels_pca(max_features=3000, n_gram=1, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "\n",
    "    \"trigram_5000features\": list(get_training_labels_pca(max_features=5000, n_gram=3, emotion=True, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"trigram_5000features_noEmotion\": list(get_training_labels_pca(max_features=5000, n_gram=3, emotion=False, sentiment=True, utterance_2=True, utterance_3=True)),\n",
    "    \"trigram_5000features_noEmotion_noSentiment\": list(get_training_labels_pca(max_features=5000, n_gram=3, emotion=False, sentiment=False, utterance_2=True, utterance_3=True))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2f00e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
